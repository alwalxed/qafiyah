{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Arabic Poem Embedding Pipeline\n",
    "\n",
    "**Disclaimer:** This is *not* a production-ready pipeline — it’s the one used for our initial verse-embedding experiments in the Qafiyah project.\n",
    "\n",
    "This notebook connects to your PostgreSQL `poems` database, denormalizes records,\n",
    "splits Arabic verses (with optional diacritic removal), embeds each verse using\n",
    "a Hugging Face model, and writes out Parquet batches—then merges them into one file.\n",
    "\n",
    "**Before you run anything:** edit the **Configuration** cell with your\n",
    "database credentials and desired settings."
   ],
   "id": "83eedd02df015831"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install sqlalchemy pandas torch transformers pyarrow psycopg2-binary tqdm\n",
   "id": "39a4380a773255c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow.parquet import ParquetWriter\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from sqlalchemy import create_engine, text\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ],
   "id": "9f5c91b5b04e405",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Database credentials\n",
    "DB_USER = \"your_db_user\"\n",
    "DB_PASSWORD = \"your_db_password\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"qafiyah_dev\"\n",
    "\n",
    "# Embedding model & batching\n",
    "# With Transformers versions earlier than 4.51.0, you may encounter the following error: KeyError: 'qwen3'\n",
    "MODEL_NAME = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "MAX_LENGTH = 8192  # from Qwen docs; kindly do not change it\n",
    "BATCH_SIZE = 256\n",
    "SAVE_DIR = Path(\"verse_parquet_batches\")\n",
    "\n",
    "# Device selection: prefer CUDA, then MPS, then CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using Apple MPS\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "logger = logging.getLogger()\n"
   ],
   "id": "18d97839bbd77ba1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create the SQLAlchemy engine, count total poems, then build a `denormalized_poems` table.\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\",\n",
    "    future=True\n",
    ")\n",
    "\n",
    "# Count poems & verses\n",
    "with engine.begin() as conn:\n",
    "    result = conn.execute(text(\"\"\"\n",
    "                               SELECT COUNT(*) AS total_poems, SUM(num_verses) AS total_verses\n",
    "                               FROM poems\n",
    "                               \"\"\"))\n",
    "    total_poems, total_verses = result.fetchone()\n",
    "\n",
    "print(f\"Total poems: {total_poems:,}\")\n",
    "print(f\"Total verses (sum of num_verses column): {total_verses:,}\")\n"
   ],
   "id": "9645b1898960637c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Denormalize into a single table\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS denormalized_poems AS\n",
    "        SELECT\n",
    "            poems.id AS poem_id,\n",
    "            poems.title,\n",
    "            poems.content,\n",
    "            poets.id AS poet_id,\n",
    "            poets.name AS poet_name,\n",
    "            eras.name AS era_name,\n",
    "            meters.name AS meter_name,\n",
    "            themes.name AS theme_name,\n",
    "            rhymes.pattern AS rhyme_pattern,\n",
    "            types.name AS type_name\n",
    "        FROM poems\n",
    "        LEFT JOIN poets     ON poems.poet_id = poets.id\n",
    "        LEFT JOIN eras      ON poets.era_id = eras.id\n",
    "        LEFT JOIN meters    ON poems.meter_id = meters.id\n",
    "        LEFT JOIN themes    ON poems.theme_id = themes.id\n",
    "        LEFT JOIN rhymes    ON poems.rhyme_id = rhymes.id\n",
    "        LEFT JOIN types     ON poems.type_id = types.id;\n",
    "    \"\"\"))\n",
    "\n",
    "logger.info(\"Denormalized table ready.\")\n"
   ],
   "id": "5e219fc5cb740444",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read the denormalized table into a DataFrame. Optionally, create a clean column without Arabic tashkeel.\n",
    "\n",
    "df = pd.read_sql(\"denormalized_poems\", engine)[[\"poem_id\", \"content\"]]\n",
    "\n",
    "arabic_diacritics = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "\n",
    "\n",
    "def remove_tashkeel(text):\n",
    "    return arabic_diacritics.sub(\"\", text) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# Add an optional clean column (without diacritics)\n",
    "df[\"content_no_diacritics\"] = df[\"content\"].apply(remove_tashkeel)\n",
    "\n",
    "logger.info(\"Loaded %d poems into DataFrame.\", len(df))\n"
   ],
   "id": "ee8855effd433870",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split on `*` markers into hemistich pairs.\n",
    "def split_poem_into_verses(poem_id, content):\n",
    "    if not isinstance(content, str) or not content.strip():\n",
    "        return []\n",
    "    parts = [p.strip() for p in content.split(\"*\") if p.strip()]\n",
    "    verses = []\n",
    "    for i in range(0, len(parts) - 1, 2):\n",
    "        verses.append({\n",
    "            \"poem_id\": poem_id,\n",
    "            \"verse\": f\"{parts[i]} {parts[i + 1]}\"\n",
    "        })\n",
    "    return verses\n"
   ],
   "id": "52c5973673a404c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def last_token_pool(\n",
    "    last_hidden_states: Tensor,\n",
    "    attention_mask: Tensor\n",
    ") -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[\n",
    "            torch.arange(batch_size, device=last_hidden_states.device),\n",
    "            sequence_lengths\n",
    "        ]\n",
    "\n",
    "\n",
    "def prepare_embedding_batches(verses_df: pd.DataFrame, save_dir: Path):\n",
    "    logger.info(\"Embedding %d verses…\", len(verses_df))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "    model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE).eval()\n",
    "    eod_token = tokenizer.eos_token or \"<|endoftext|>\"\n",
    "\n",
    "    save_dir.mkdir(exist_ok=True, parents=True)\n",
    "    batch_idx = 0\n",
    "\n",
    "    with tqdm(total=len(verses_df), desc=\"Verses\", unit=\"v\") as pbar:\n",
    "        for start in range(0, len(verses_df), BATCH_SIZE):\n",
    "            batch = verses_df.iloc[start: start + BATCH_SIZE]\n",
    "            texts = [v + eod_token for v in batch[\"verse\"]]\n",
    "\n",
    "            enc = tokenizer(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(**enc)\n",
    "            embeds = last_token_pool(out.last_hidden_state, enc[\"attention_mask\"])\n",
    "            embeds = F.normalize(embeds, p=2, dim=1).cpu()\n",
    "\n",
    "            records = []\n",
    "            for i, (_, row) in enumerate(batch.iterrows()):\n",
    "                records.append({\n",
    "                    \"poem_id\": row[\"poem_id\"],\n",
    "                    \"verse\": row[\"verse\"],\n",
    "                    \"embedding\": embeds[i].tolist()\n",
    "                })\n",
    "\n",
    "            df_batch = pd.DataFrame(records)\n",
    "            pq.write_table(\n",
    "                pa.Table.from_pandas(df_batch),\n",
    "                save_dir / f\"batch_{batch_idx:05d}.parquet\",\n",
    "                compression=\"snappy\"\n",
    "            )\n",
    "            batch_idx += 1\n",
    "            pbar.update(len(batch))\n",
    "\n",
    "    logger.info(\"Wrote %d batches to `%s`\", batch_idx, str(save_dir))\n"
   ],
   "id": "e60ab952b9bb2a8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pull verses from the cleaned column and execute batching. Change the column name if you want to use the original content column with tashkeel.\n",
    "\n",
    "all_verses = []\n",
    "for _, row in df.iterrows():\n",
    "    all_verses.extend(\n",
    "        split_poem_into_verses(row[\"poem_id\"], row[\"content_no_diacritics\"])\n",
    "    )\n",
    "verses_df = pd.DataFrame(all_verses)\n",
    "prepare_embedding_batches(verses_df, SAVE_DIR)\n"
   ],
   "id": "66332580bde14306",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def merge_parquet_batches(input_dir: Path, output_file: Path):\n",
    "    files = sorted(input_dir.glob(\"*.parquet\"))\n",
    "    writer = None\n",
    "\n",
    "    for path in files:\n",
    "        table = pq.read_table(path)  # load one batch at a time\n",
    "        if writer is None:\n",
    "            # initialize writer with schema of first batch\n",
    "            writer = ParquetWriter(str(output_file), table.schema, compression=\"zstd\")\n",
    "        writer.write_table(table)\n",
    "\n",
    "    if writer:\n",
    "        writer.close()\n",
    "    logger.info(\"Merged %d files → %s\", len(files), output_file)\n",
    "\n",
    "merge_parquet_batches(SAVE_DIR, Path(\"verse_embeddings.parquet\"))\n"
   ],
   "id": "138b1cb98dc81895",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inspect Results\n",
    "\n",
    "df_emb = pd.read_parquet(\"verse_embeddings.parquet\")\n",
    "print(\"Total verses:\", len(df_emb))\n",
    "print(\"Columns:\", df_emb.columns.tolist())\n",
    "df_emb.head()\n"
   ],
   "id": "832f6d0b950eed21",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
